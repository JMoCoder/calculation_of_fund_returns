# 收益分配测算系统 - 数据库设计

## 1. 数据库设计概述

### 1.1 设计原则
- **数据安全**: 不持久化敏感业务数据，确保用户数据安全
- **临时存储**: 仅用于会话期间的数据缓存和临时存储
- **高性能**: 优化数据结构，提供快速读写性能
- **自动清理**: 实现数据的自动过期和清理机制
- **可扩展**: 支持系统功能扩展的数据结构设计

### 1.2 存储策略

#### 数据分类
```
┌─────────────────────────────────────────────────────────┐
│                    数据分类                              │
├─────────────────────────────────────────────────────────┤
│ 1. 会话数据 (Session Data)                              │
│    - 用户会话信息                                        │
│    - 临时计算参数                                        │
│    - 中间计算结果                                        │
│                                                         │
│ 2. 缓存数据 (Cache Data)                                │
│    - 计算结果缓存                                        │
│    - 文件处理缓存                                        │
│    - API响应缓存                                         │
│                                                         │
│ 3. 临时文件 (Temporary Files)                           │
│    - 上传的Excel文件                                     │
│    - 生成的导出文件                                      │
│    - 图表图片文件                                        │
│                                                         │
│ 4. 系统配置 (System Config)                             │
│    - 应用配置参数                                        │
│    - 计算模板配置                                        │
│    - 系统运行状态                                        │
└─────────────────────────────────────────────────────────┘
```

## 2. Redis数据结构设计

### 2.1 会话数据结构

#### 会话基本信息
```redis
# Key: session:{session_id}
# Type: Hash
# TTL: 7200 seconds (2 hours)

HMSET session:550e8400-e29b-41d4-a716-446655440000
  id "550e8400-e29b-41d4-a716-446655440000"
  created_at "2024-01-15T10:30:00Z"
  last_accessed "2024-01-15T12:15:30Z"
  user_agent "Mozilla/5.0 (Windows NT 10.0; Win64; x64)..."
  client_ip "192.168.1.100"
  status "active"
```

#### 计算参数存储
```redis
# Key: session:{session_id}:params
# Type: String (JSON)
# TTL: 7200 seconds

SET session:550e8400-e29b-41d4-a716-446655440000:params
'{
  "basic_params": {
    "investment_target": "某地产项目",
    "investment_amount": 10000.0,
    "investment_period": 5,
    "hurdle_rate": 8.0,
    "carry_rate": 20.0
  },
  "cash_flows": [
    {"year": 1, "amount": 1000.0},
    {"year": 2, "amount": 2000.0},
    {"year": 3, "amount": 3000.0},
    {"year": 4, "amount": 4000.0},
    {"year": 5, "amount": 5000.0}
  ],
  "distribution_params": {
    "mode": "flat_principal_first"
  }
}'
EX 7200
```

#### 计算结果存储
```redis
# Key: session:{session_id}:results
# Type: String (JSON)
# TTL: 7200 seconds

SET session:550e8400-e29b-41d4-a716-446655440000:results
'{
  "irr": 15.24,
  "dpi": 1.50,
  "calculation_time": 0.125,
  "cash_flow_details": [
    {
      "year": 1,
      "net_cash_flow": 1000.0,
      "principal_repayment": 1000.0,
      "remaining_principal": 9000.0,
      "hurdle_accrual": 800.0,
      "hurdle_distribution": 0.0,
      "carry_lp": 0.0,
      "carry_gp": 0.0
    }
  ],
  "summary": {
    "total_distributions": 15000.0,
    "total_carry": 1200.0,
    "final_irr": 15.24
  }
}'
EX 7200
```

### 2.2 缓存数据结构

#### 计算结果缓存
```redis
# Key: cache:calculation:{hash}
# Type: String (JSON)
# TTL: 3600 seconds (1 hour)

# 使用参数哈希作为缓存键
SET cache:calculation:a1b2c3d4e5f6...
'{
  "result": {...},
  "cached_at": "2024-01-15T12:15:30Z",
  "cache_version": "1.0"
}'
EX 3600
```

#### 文件处理缓存
```redis
# Key: cache:file:{file_hash}
# Type: Hash
# TTL: 1800 seconds (30 minutes)

HMSET cache:file:sha256_hash_of_file
  original_name "investment_data.xlsx"
  file_size "2048"
  processed_at "2024-01-15T12:15:30Z"
  parsed_data '{"basic_params": {...}, "cash_flows": [...]}'
  status "processed"
EX 1800
```

### 2.3 系统配置数据

#### 应用配置
```redis
# Key: config:app
# Type: Hash
# TTL: No expiration

HMSET config:app
  version "1.0.0"
  max_investment_period "30"
  max_file_size "10485760"
  session_timeout "7200"
  cache_timeout "3600"
  supported_file_types "xlsx,xls"
```

#### 计算模板配置
```redis
# Key: config:calculation_templates
# Type: String (JSON)
# TTL: No expiration

SET config:calculation_templates
'{
  "flat_structures": {
    "principal_first": {
      "name": "优先还本",
      "description": "净现金流优先还本，再分配收益",
      "required_params": ["hurdle_rate", "carry_rate"]
    },
    "periodic": {
      "name": "期间分配",
      "description": "先分配期间收益，再还本",
      "required_params": ["hurdle_rate", "carry_rate", "periodic_rate"]
    }
  },
  "structured": {
    "senior_subordinate": {
      "name": "优先劣后",
      "description": "优先级和劣后级分层结构",
      "required_params": ["senior_ratio", "subordinate_ratio"]
    }
  }
}'
```

## 3. 文件存储设计

### 3.1 临时文件存储

#### 目录结构
```
temp/
├── uploads/                    # 上传文件目录
│   ├── {session_id}/          # 按会话分组
│   │   ├── {timestamp}_{filename}.xlsx
│   │   └── metadata.json      # 文件元数据
│   └── cleanup.log            # 清理日志
├── exports/                   # 导出文件目录
│   ├── {session_id}/          # 按会话分组
│   │   ├── results_{timestamp}.xlsx
│   │   ├── charts_{timestamp}.png
│   │   └── metadata.json
│   └── cleanup.log
└── cache/                     # 文件缓存目录
    ├── parsed/                # 解析结果缓存
    ├── generated/             # 生成文件缓存
    └── cleanup.log
```

#### 文件元数据结构
```json
{
  "file_id": "f47ac10b-58cc-4372-a567-0e02b2c3d479",
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "original_name": "investment_data.xlsx",
  "stored_name": "20240115_121530_investment_data.xlsx",
  "file_size": 2048,
  "mime_type": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
  "upload_time": "2024-01-15T12:15:30Z",
  "expiry_time": "2024-01-15T14:15:30Z",
  "checksum": "sha256:a1b2c3d4e5f6...",
  "status": "uploaded",
  "processing_status": "pending"
}
```

### 3.2 文件生命周期管理

#### 文件状态流转
```
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│   uploaded  │───▶│ processing  │───▶│  processed  │
└─────────────┘    └─────────────┘    └─────────────┘
       │                   │                   │
       ▼                   ▼                   ▼
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│    error    │    │    error    │    │   expired   │
└─────────────┘    └─────────────┘    └─────────────┘
       │                   │                   │
       └───────────────────┼───────────────────┘
                           ▼
                  ┌─────────────┐
                  │   deleted   │
                  └─────────────┘
```

#### 自动清理策略
```python
# 文件清理配置
FILE_CLEANUP_RULES = {
    "uploads": {
        "max_age_hours": 2,        # 上传文件最大保存2小时
        "max_size_mb": 100,        # 单个会话最大100MB
        "cleanup_interval": 300    # 每5分钟检查一次
    },
    "exports": {
        "max_age_hours": 1,        # 导出文件最大保存1小时
        "max_size_mb": 50,         # 单个会话最大50MB
        "cleanup_interval": 600    # 每10分钟检查一次
    },
    "cache": {
        "max_age_hours": 24,       # 缓存文件最大保存24小时
        "max_size_gb": 1,          # 缓存总大小最大1GB
        "cleanup_interval": 3600   # 每小时检查一次
    }
}
```

## 4. 数据模型定义

### 4.1 会话数据模型

```python
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any
from datetime import datetime
from enum import Enum

class SessionStatus(str, Enum):
    ACTIVE = "active"
    EXPIRED = "expired"
    TERMINATED = "terminated"

class SessionData(BaseModel):
    """会话数据模型"""
    id: str = Field(..., description="会话ID")
    created_at: datetime = Field(..., description="创建时间")
    last_accessed: datetime = Field(..., description="最后访问时间")
    user_agent: Optional[str] = Field(None, description="用户代理")
    client_ip: Optional[str] = Field(None, description="客户端IP")
    status: SessionStatus = Field(SessionStatus.ACTIVE, description="会话状态")
    data: Dict[str, Any] = Field(default_factory=dict, description="会话数据")
    
    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }

class CalculationParams(BaseModel):
    """计算参数模型"""
    basic_params: Dict[str, Any] = Field(..., description="基本参数")
    cash_flows: List[Dict[str, Any]] = Field(..., description="现金流数据")
    distribution_params: Dict[str, Any] = Field(..., description="分配参数")
    created_at: datetime = Field(default_factory=datetime.utcnow)
    
class CalculationResult(BaseModel):
    """计算结果模型"""
    irr: float = Field(..., description="内部收益率")
    dpi: float = Field(..., description="分配倍数")
    cash_flow_details: List[Dict[str, Any]] = Field(..., description="现金流明细")
    summary: Dict[str, Any] = Field(..., description="结果摘要")
    calculation_time: float = Field(..., description="计算耗时")
    calculated_at: datetime = Field(default_factory=datetime.utcnow)
```

### 4.2 文件数据模型

```python
class FileStatus(str, Enum):
    UPLOADED = "uploaded"
    PROCESSING = "processing"
    PROCESSED = "processed"
    ERROR = "error"
    EXPIRED = "expired"
    DELETED = "deleted"

class FileMetadata(BaseModel):
    """文件元数据模型"""
    file_id: str = Field(..., description="文件ID")
    session_id: str = Field(..., description="会话ID")
    original_name: str = Field(..., description="原始文件名")
    stored_name: str = Field(..., description="存储文件名")
    file_size: int = Field(..., description="文件大小")
    mime_type: str = Field(..., description="MIME类型")
    upload_time: datetime = Field(..., description="上传时间")
    expiry_time: datetime = Field(..., description="过期时间")
    checksum: str = Field(..., description="文件校验和")
    status: FileStatus = Field(FileStatus.UPLOADED, description="文件状态")
    processing_status: Optional[str] = Field(None, description="处理状态")
    error_message: Optional[str] = Field(None, description="错误信息")
    
class ParsedFileData(BaseModel):
    """解析后的文件数据模型"""
    file_id: str = Field(..., description="文件ID")
    basic_params: Optional[Dict[str, Any]] = Field(None, description="基本参数")
    cash_flows: Optional[List[Dict[str, Any]]] = Field(None, description="现金流数据")
    distribution_params: Optional[Dict[str, Any]] = Field(None, description="分配参数")
    parsing_errors: List[str] = Field(default_factory=list, description="解析错误")
    parsed_at: datetime = Field(default_factory=datetime.utcnow)
```

### 4.3 缓存数据模型

```python
class CacheEntry(BaseModel):
    """缓存条目模型"""
    key: str = Field(..., description="缓存键")
    value: Any = Field(..., description="缓存值")
    created_at: datetime = Field(default_factory=datetime.utcnow)
    expires_at: Optional[datetime] = Field(None, description="过期时间")
    access_count: int = Field(default=0, description="访问次数")
    last_accessed: Optional[datetime] = Field(None, description="最后访问时间")
    
class CalculationCache(BaseModel):
    """计算结果缓存模型"""
    params_hash: str = Field(..., description="参数哈希")
    result: CalculationResult = Field(..., description="计算结果")
    cache_version: str = Field("1.0", description="缓存版本")
    hit_count: int = Field(default=0, description="命中次数")
```

## 5. 数据访问层设计

### 5.1 Redis操作封装

```python
import redis.asyncio as redis
import json
import pickle
from typing import Optional, Any, List
from datetime import datetime, timedelta

class RedisRepository:
    """Redis数据访问层"""
    
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
    
    async def set_session(self, session_id: str, session_data: SessionData, ttl: int = 7200) -> bool:
        """设置会话数据"""
        try:
            key = f"session:{session_id}"
            value = session_data.json()
            await self.redis.setex(key, ttl, value)
            return True
        except Exception as e:
            logger.error(f"设置会话数据失败: {e}")
            return False
    
    async def get_session(self, session_id: str) -> Optional[SessionData]:
        """获取会话数据"""
        try:
            key = f"session:{session_id}"
            value = await self.redis.get(key)
            if value:
                data = json.loads(value)
                return SessionData(**data)
            return None
        except Exception as e:
            logger.error(f"获取会话数据失败: {e}")
            return None
    
    async def set_calculation_params(self, session_id: str, params: CalculationParams, ttl: int = 7200) -> bool:
        """设置计算参数"""
        try:
            key = f"session:{session_id}:params"
            value = params.json()
            await self.redis.setex(key, ttl, value)
            return True
        except Exception as e:
            logger.error(f"设置计算参数失败: {e}")
            return False
    
    async def get_calculation_params(self, session_id: str) -> Optional[CalculationParams]:
        """获取计算参数"""
        try:
            key = f"session:{session_id}:params"
            value = await self.redis.get(key)
            if value:
                data = json.loads(value)
                return CalculationParams(**data)
            return None
        except Exception as e:
            logger.error(f"获取计算参数失败: {e}")
            return None
    
    async def set_calculation_result(self, session_id: str, result: CalculationResult, ttl: int = 7200) -> bool:
        """设置计算结果"""
        try:
            key = f"session:{session_id}:results"
            value = result.json()
            await self.redis.setex(key, ttl, value)
            return True
        except Exception as e:
            logger.error(f"设置计算结果失败: {e}")
            return False
    
    async def get_calculation_result(self, session_id: str) -> Optional[CalculationResult]:
        """获取计算结果"""
        try:
            key = f"session:{session_id}:results"
            value = await self.redis.get(key)
            if value:
                data = json.loads(value)
                return CalculationResult(**data)
            return None
        except Exception as e:
            logger.error(f"获取计算结果失败: {e}")
            return None
    
    async def set_cache(self, cache_key: str, data: Any, ttl: int = 3600) -> bool:
        """设置缓存数据"""
        try:
            key = f"cache:{cache_key}"
            value = pickle.dumps(data)
            await self.redis.setex(key, ttl, value)
            return True
        except Exception as e:
            logger.error(f"设置缓存数据失败: {e}")
            return False
    
    async def get_cache(self, cache_key: str) -> Optional[Any]:
        """获取缓存数据"""
        try:
            key = f"cache:{cache_key}"
            value = await self.redis.get(key)
            if value:
                return pickle.loads(value)
            return None
        except Exception as e:
            logger.error(f"获取缓存数据失败: {e}")
            return None
    
    async def delete_session_data(self, session_id: str) -> bool:
        """删除会话相关的所有数据"""
        try:
            pattern = f"session:{session_id}*"
            keys = await self.redis.keys(pattern)
            if keys:
                await self.redis.delete(*keys)
            return True
        except Exception as e:
            logger.error(f"删除会话数据失败: {e}")
            return False
    
    async def get_active_sessions(self) -> List[str]:
        """获取活跃会话列表"""
        try:
            pattern = "session:*"
            keys = await self.redis.keys(pattern)
            session_ids = []
            for key in keys:
                if key.count(':') == 1:  # 只获取基本会话键
                    session_id = key.split(':')[1]
                    session_ids.append(session_id)
            return session_ids
        except Exception as e:
            logger.error(f"获取活跃会话失败: {e}")
            return []
```

### 5.2 文件存储操作

```python
import os
import shutil
import hashlib
from pathlib import Path
from typing import Optional, List

class FileRepository:
    """文件存储访问层"""
    
    def __init__(self, base_path: str):
        self.base_path = Path(base_path)
        self.uploads_path = self.base_path / "uploads"
        self.exports_path = self.base_path / "exports"
        self.cache_path = self.base_path / "cache"
        
        # 确保目录存在
        for path in [self.uploads_path, self.exports_path, self.cache_path]:
            path.mkdir(parents=True, exist_ok=True)
    
    async def save_upload_file(self, session_id: str, filename: str, content: bytes) -> Optional[FileMetadata]:
        """保存上传文件"""
        try:
            # 创建会话目录
            session_dir = self.uploads_path / session_id
            session_dir.mkdir(exist_ok=True)
            
            # 生成文件ID和存储名称
            file_id = self._generate_file_id()
            timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
            stored_name = f"{timestamp}_{filename}"
            file_path = session_dir / stored_name
            
            # 保存文件
            with open(file_path, 'wb') as f:
                f.write(content)
            
            # 计算校验和
            checksum = self._calculate_checksum(content)
            
            # 创建元数据
            metadata = FileMetadata(
                file_id=file_id,
                session_id=session_id,
                original_name=filename,
                stored_name=stored_name,
                file_size=len(content),
                mime_type=self._get_mime_type(filename),
                upload_time=datetime.utcnow(),
                expiry_time=datetime.utcnow() + timedelta(hours=2),
                checksum=checksum
            )
            
            # 保存元数据
            metadata_path = session_dir / "metadata.json"
            await self._save_metadata(metadata_path, metadata)
            
            return metadata
            
        except Exception as e:
            logger.error(f"保存上传文件失败: {e}")
            return None
    
    async def get_file_content(self, session_id: str, file_id: str) -> Optional[bytes]:
        """获取文件内容"""
        try:
            metadata = await self.get_file_metadata(session_id, file_id)
            if not metadata:
                return None
            
            file_path = self.uploads_path / session_id / metadata.stored_name
            if file_path.exists():
                with open(file_path, 'rb') as f:
                    return f.read()
            return None
            
        except Exception as e:
            logger.error(f"获取文件内容失败: {e}")
            return None
    
    async def save_export_file(self, session_id: str, filename: str, content: bytes) -> Optional[str]:
        """保存导出文件"""
        try:
            # 创建会话目录
            session_dir = self.exports_path / session_id
            session_dir.mkdir(exist_ok=True)
            
            # 生成文件名
            timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
            stored_name = f"{timestamp}_{filename}"
            file_path = session_dir / stored_name
            
            # 保存文件
            with open(file_path, 'wb') as f:
                f.write(content)
            
            return str(file_path)
            
        except Exception as e:
            logger.error(f"保存导出文件失败: {e}")
            return None
    
    async def cleanup_expired_files(self) -> int:
        """清理过期文件"""
        cleaned_count = 0
        current_time = datetime.utcnow()
        
        try:
            # 清理上传文件
            for session_dir in self.uploads_path.iterdir():
                if session_dir.is_dir():
                    metadata_path = session_dir / "metadata.json"
                    if metadata_path.exists():
                        metadata = await self._load_metadata(metadata_path)
                        if metadata and current_time > metadata.expiry_time:
                            shutil.rmtree(session_dir)
                            cleaned_count += 1
            
            # 清理导出文件（1小时过期）
            for session_dir in self.exports_path.iterdir():
                if session_dir.is_dir():
                    for file_path in session_dir.iterdir():
                        if file_path.is_file():
                            file_age = current_time - datetime.fromtimestamp(file_path.stat().st_mtime)
                            if file_age > timedelta(hours=1):
                                file_path.unlink()
                                cleaned_count += 1
            
            return cleaned_count
            
        except Exception as e:
            logger.error(f"清理过期文件失败: {e}")
            return 0
    
    def _generate_file_id(self) -> str:
        """生成文件ID"""
        import uuid
        return str(uuid.uuid4())
    
    def _calculate_checksum(self, content: bytes) -> str:
        """计算文件校验和"""
        return f"sha256:{hashlib.sha256(content).hexdigest()}"
    
    def _get_mime_type(self, filename: str) -> str:
        """获取MIME类型"""
        ext = os.path.splitext(filename)[1].lower()
        mime_types = {
            '.xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            '.xls': 'application/vnd.ms-excel'
        }
        return mime_types.get(ext, 'application/octet-stream')
```

## 6. 数据清理和维护

### 6.1 自动清理任务

```python
import asyncio
from datetime import datetime, timedelta
from typing import Dict, Any

class DataCleanupService:
    """数据清理服务"""
    
    def __init__(self, redis_repo: RedisRepository, file_repo: FileRepository):
        self.redis_repo = redis_repo
        self.file_repo = file_repo
        self.cleanup_stats = {
            'sessions_cleaned': 0,
            'files_cleaned': 0,
            'cache_cleaned': 0,
            'last_cleanup': None
        }
    
    async def run_cleanup_tasks(self) -> Dict[str, Any]:
        """运行清理任务"""
        logger.info("开始执行数据清理任务")
        
        # 清理过期会话
        sessions_cleaned = await self._cleanup_expired_sessions()
        
        # 清理过期文件
        files_cleaned = await self.file_repo.cleanup_expired_files()
        
        # 清理过期缓存
        cache_cleaned = await self._cleanup_expired_cache()
        
        # 更新统计信息
        self.cleanup_stats.update({
            'sessions_cleaned': sessions_cleaned,
            'files_cleaned': files_cleaned,
            'cache_cleaned': cache_cleaned,
            'last_cleanup': datetime.utcnow().isoformat()
        })
        
        logger.info(f"数据清理完成: {self.cleanup_stats}")
        return self.cleanup_stats
    
    async def _cleanup_expired_sessions(self) -> int:
        """清理过期会话"""
        try:
            active_sessions = await self.redis_repo.get_active_sessions()
            cleaned_count = 0
            
            for session_id in active_sessions:
                session_data = await self.redis_repo.get_session(session_id)
                if session_data:
                    # 检查会话是否过期（2小时无活动）
                    last_accessed = session_data.last_accessed
                    if datetime.utcnow() - last_accessed > timedelta(hours=2):
                        await self.redis_repo.delete_session_data(session_id)
                        cleaned_count += 1
            
            return cleaned_count
            
        except Exception as e:
            logger.error(f"清理过期会话失败: {e}")
            return 0
    
    async def _cleanup_expired_cache(self) -> int:
        """清理过期缓存"""
        try:
            # Redis会自动清理过期键，这里主要是统计
            # 可以添加额外的缓存清理逻辑
            return 0
            
        except Exception as e:
            logger.error(f"清理过期缓存失败: {e}")
            return 0
    
    async def force_cleanup_session(self, session_id: str) -> bool:
        """强制清理指定会话"""
        try:
            # 删除Redis中的会话数据
            await self.redis_repo.delete_session_data(session_id)
            
            # 删除文件系统中的会话文件
            session_upload_dir = self.file_repo.uploads_path / session_id
            session_export_dir = self.file_repo.exports_path / session_id
            
            if session_upload_dir.exists():
                shutil.rmtree(session_upload_dir)
            
            if session_export_dir.exists():
                shutil.rmtree(session_export_dir)
            
            return True
            
        except Exception as e:
            logger.error(f"强制清理会话失败: {e}")
            return False
```

### 6.2 定时清理任务

```python
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger

class ScheduledCleanupService:
    """定时清理服务"""
    
    def __init__(self, cleanup_service: DataCleanupService):
        self.cleanup_service = cleanup_service
        self.scheduler = AsyncIOScheduler()
    
    def start_scheduled_cleanup(self):
        """启动定时清理任务"""
        # 每30分钟执行一次清理任务
        self.scheduler.add_job(
            self.cleanup_service.run_cleanup_tasks,
            trigger=IntervalTrigger(minutes=30),
            id='data_cleanup',
            name='数据清理任务',
            replace_existing=True
        )
        
        # 每天凌晨2点执行深度清理
        self.scheduler.add_job(
            self._deep_cleanup,
            trigger='cron',
            hour=2,
            minute=0,
            id='deep_cleanup',
            name='深度清理任务',
            replace_existing=True
        )
        
        self.scheduler.start()
        logger.info("定时清理任务已启动")
    
    async def _deep_cleanup(self):
        """深度清理任务"""
        logger.info("开始执行深度清理任务")
        
        # 执行常规清理
        await self.cleanup_service.run_cleanup_tasks()
        
        # 清理缓存目录
        cache_path = self.cleanup_service.file_repo.cache_path
        if cache_path.exists():
            for file_path in cache_path.rglob('*'):
                if file_path.is_file():
                    file_age = datetime.utcnow() - datetime.fromtimestamp(file_path.stat().st_mtime)
                    if file_age > timedelta(hours=24):
                        file_path.unlink()
        
        logger.info("深度清理任务完成")
    
    def stop_scheduled_cleanup(self):
        """停止定时清理任务"""
        self.scheduler.shutdown()
        logger.info("定时清理任务已停止")
```

## 7. 数据安全和备份

### 7.1 数据安全策略

```python
class DataSecurityService:
    """数据安全服务"""
    
    def __init__(self, encryption_key: str):
        self.fernet = Fernet(encryption_key.encode())
    
    def encrypt_sensitive_data(self, data: str) -> str:
        """加密敏感数据"""
        return self.fernet.encrypt(data.encode()).decode()
    
    def decrypt_sensitive_data(self, encrypted_data: str) -> str:
        """解密敏感数据"""
        return self.fernet.decrypt(encrypted_data.encode()).decode()
    
    def anonymize_session_data(self, session_data: SessionData) -> SessionData:
        """匿名化会话数据"""
        # 移除敏感信息
        anonymized = session_data.copy()
        anonymized.client_ip = self._anonymize_ip(session_data.client_ip)
        anonymized.user_agent = "[ANONYMIZED]"
        return anonymized
    
    def _anonymize_ip(self, ip: str) -> str:
        """匿名化IP地址"""
        if not ip:
            return ip
        parts = ip.split('.')
        if len(parts) == 4:
            return f"{parts[0]}.{parts[1]}.xxx.xxx"
        return "[ANONYMIZED]"
```

### 7.2 数据审计

```python
class DataAuditService:
    """数据审计服务"""
    
    def __init__(self, redis_repo: RedisRepository):
        self.redis_repo = redis_repo
    
    async def generate_audit_report(self) -> Dict[str, Any]:
        """生成审计报告"""
        report = {
            'timestamp': datetime.utcnow().isoformat(),
            'active_sessions': 0,
            'total_calculations': 0,
            'cache_usage': {},
            'file_storage': {},
            'security_events': []
        }
        
        try:
            # 统计活跃会话
            active_sessions = await self.redis_repo.get_active_sessions()
            report['active_sessions'] = len(active_sessions)
            
            # 统计计算次数
            # 这里可以添加计算统计逻辑
            
            # 统计缓存使用情况
            # 这里可以添加缓存统计逻辑
            
            return report
            
        except Exception as e:
            logger.error(f"生成审计报告失败: {e}")
            return report
```

## 8. 性能监控

### 8.1 数据库性能监控

```python
class DatabaseMonitoringService:
    """数据库监控服务"""
    
    def __init__(self, redis_repo: RedisRepository):
        self.redis_repo = redis_repo
        self.metrics = {
            'redis_connections': 0,
            'redis_memory_usage': 0,
            'cache_hit_rate': 0.0,
            'average_response_time': 0.0
        }
    
    async def collect_metrics(self) -> Dict[str, Any]:
        """收集性能指标"""
        try:
            # Redis连接信息
            info = await self.redis_repo.redis.info()
            
            self.metrics.update({
                'redis_connections': info.get('connected_clients', 0),
                'redis_memory_usage': info.get('used_memory', 0),
                'redis_keyspace_hits': info.get('keyspace_hits', 0),
                'redis_keyspace_misses': info.get('keyspace_misses', 0)
            })
            
            # 计算缓存命中率
            hits = self.metrics['redis_keyspace_hits']
            misses = self.metrics['redis_keyspace_misses']
            if hits + misses > 0:
                self.metrics['cache_hit_rate'] = hits / (hits + misses)
            
            return self.metrics
            
        except Exception as e:
            logger.error(f"收集性能指标失败: {e}")
            return self.metrics
```

这个数据库设计文档详细说明了系统的数据存储策略、数据模型、访问层设计以及数据安全和维护方案，确保系统在不持久化敏感数据的前提下，提供高效、安全的数据管理能力。